# 知乎热点问题采集系统使用说明

## 系统概述

本系统可以自动采集知乎创作者中心热点问题页面(https://www.zhihu.com/creator/hot-question/hot/0/hour)的数据，并存储到本地SQLite数据库中，同时提供简单的Web界面查看采集的数据。

## 前置条件

1. 安装Python 3.6或更高版本
2. 安装项目依赖：
   ```bash
   pip install -r requirements.txt
   ```
3. 安装Playwright浏览器：
   ```bash
   playwright install
   ```
4. 准备知乎账号Cookies：
   - 登录知乎网站
   - 打开浏览器开发者工具(F12)
   - 在网络面板中查看任意请求的Cookie
   - 复制完整的Cookie字符串

## 配置系统

1. 打开`config/config.json`文件
2. 将准备好的Cookie字符串填入"cookies"字段
3. 根据需要调整其他配置项：
   - `scrape_count`：每次采集的问题数量（默认20个）
   - `scrape_interval`：采集间隔时间（秒，默认3600秒即1小时）
   - `scroll_count`：页面滚动次数（默认3次）
   - `scroll_interval`：滚动间隔时间（毫秒，默认1000毫秒）

## 运行系统

本系统提供多种运行方式：

### 完整模式（爬虫+Web服务）

```bash
python run.py
```

这将启动爬虫任务采集数据，并同时启动Web服务，可通过浏览器访问http://localhost:5000查看数据。

### 仅爬虫模式

```bash
python run.py --scrape-only
```

只执行一次爬虫任务，采集数据并保存到数据库，不启动Web服务。

### 仅Web服务模式

```bash
python run.py --web-only
```

只启动Web服务，不执行爬虫任务，可用于查看已采集的数据。

### 更多命令行选项

```bash
python run.py --help
```

查看所有可用的命令行选项。

## Web界面使用

1. 访问http://localhost:5000打开Web界面
2. 主页显示已采集的热点问题列表
3. 点击右上角"配置"按钮可以修改系统配置

## 常见问题

### 爬虫无法正常工作

1. **Cookie过期**：知乎的Cookie有时效性，如果爬虫无法正常工作，可能需要更新Cookie。
2. **页面验证码**：知乎可能会弹出验证码，当前版本无法自动处理验证码，需要手动处理。
3. **网络问题**：检查网络连接是否正常。

### 数据库问题

如果遇到数据库相关问题，可以尝试删除`database/zhihu_hot_questions.db`文件，系统会自动创建新的数据库。

## 目录结构

```
zhihu-crawler/
├── config/                 # 配置文件目录
│   └── config.json        # 配置文件
├── database/               # 数据库模块
│   └── __init__.py        # 数据库类
├── scraper/                # 爬虫模块
│   ├── __init__.py        # 爬虫模块初始化
│   └── crawler.py         # 爬虫实现
├── web/                    # Web界面模块
│   ├── __init__.py        # Web界面初始化
│   ├── routes.py          # Web路由
│   └── templates/         # 模板目录
│       └── index.html     # 首页模板
├── screenshots/            # 截图目录（调试用）
├── app.py                  # 主应用入口（使用APScheduler）
├── run.py                  # 简化版入口（推荐使用）
├── config.py               # 配置管理类
├── requirements.txt        # 依赖列表
├── test_crawler.py         # 测试模块
├── test_simple.py          # 简化版测试模块
└── README.md               # 项目说明
```

## 注意事项

- 爬虫需要有效的知乎登录Cookies才能正常工作
- 请合理设置采集频率，避免对知乎服务器造成过大压力
- 采集的数据仅用于个人学习和分析，请遵守知乎的使用条款 